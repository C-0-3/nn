{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e75cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3be58a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#AND OR XOR with Sigmoid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# AND dataset\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])  # AND truth table\n",
    "\n",
    "# OR dataset\n",
    "X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_or = np.array([0, 1, 1, 1])  # OR truth table\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])  # XOR truth table\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Sigmoid derivative for backpropagation\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# MLP Class with Backpropagation\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = sigmoid(self.hidden_input)\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output = sigmoid(self.output_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        # Calculate the error\n",
    "        output_error = y - output\n",
    "        output_delta = output_error * sigmoid_derivative(output)\n",
    "        \n",
    "        # Backpropagate to hidden layer\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_input_hidden += X.T.dot(hidden_delta)\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta)\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True)\n",
    "        \n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "# Train and evaluate the AND dataset\n",
    "mlp_and = MLP(input_size=2, hidden_size=2, output_size=1)\n",
    "mlp_and.train(X_and, y_and.reshape(-1, 1), epochs=10000)\n",
    "\n",
    "# Prediction\n",
    "y_pred_and = mlp_and.predict(X_and)\n",
    "y_pred_and = np.round(y_pred_and)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy_and = accuracy_score(y_and, y_pred_and)\n",
    "print(f\"Accuracy on AND dataset: {accuracy_and * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_and = confusion_matrix(y_and, y_pred_and)\n",
    "print(f\"Confusion Matrix for AND dataset:\\n\", cm_and)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm_and, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.title(\"Confusion Matrix for AND dataset\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Train and evaluate the OR dataset\n",
    "mlp_or = MLP(input_size=2, hidden_size=2, output_size=1)\n",
    "mlp_or.train(X_or, y_or.reshape(-1, 1), epochs=10000)\n",
    "\n",
    "# Prediction\n",
    "y_pred_or = mlp_or.predict(X_or)\n",
    "y_pred_or = np.round(y_pred_or)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy_or = accuracy_score(y_or, y_pred_or)\n",
    "print(f\"Accuracy on OR dataset: {accuracy_or * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_or = confusion_matrix(y_or, y_pred_or)\n",
    "print(f\"Confusion Matrix for OR dataset:\\n\", cm_or)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm_or, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.title(\"Confusion Matrix for OR dataset\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Train and evaluate the XOR dataset\n",
    "mlp_xor = MLP(input_size=2, hidden_size=2, output_size=1)\n",
    "mlp_xor.train(X_xor, y_xor.reshape(-1, 1), epochs=10000)\n",
    "\n",
    "# Prediction\n",
    "y_pred_xor = mlp_xor.predict(X_xor)\n",
    "y_pred_xor = np.round(y_pred_xor)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy_xor = accuracy_score(y_xor, y_pred_xor)\n",
    "print(f\"Accuracy on XOR dataset: {accuracy_xor * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_xor = confusion_matrix(y_xor, y_pred_xor)\n",
    "print(f\"Confusion Matrix for XOR dataset:\\n\", cm_xor)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm_xor, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.title(\"Confusion Matrix for XOR dataset\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a43982",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Wine with ReLu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.datasets import load_wine\n",
    "import seaborn as sns\n",
    "\n",
    "# ReLU activation function and derivative\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Softmax activation function for output layer\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Softmax derivative for output layer\n",
    "def softmax_derivative(y):\n",
    "    # For simplicity, we'll use the derivative of the cross-entropy loss with softmax\n",
    "    # Since we're not using it directly in backprop, we'll focus on the cross-entropy loss instead\n",
    "    pass\n",
    "\n",
    "# Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred))\n",
    "\n",
    "# Cross-entropy derivative\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "# MLP Class with Backpropagation\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = relu(self.hidden_input)\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        # Using softmax for output layer\n",
    "        self.output = softmax(self.output_input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        # Cross-entropy derivative\n",
    "        output_error = cross_entropy_derivative(y, output)\n",
    "        \n",
    "        # Output delta\n",
    "        output_delta = output_error\n",
    "        \n",
    "        # Hidden error\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        \n",
    "        # Hidden delta\n",
    "        hidden_delta = hidden_error * relu_derivative(self.hidden_output)\n",
    "        \n",
    "        self.weights_input_hidden += X.T.dot(hidden_delta)\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta)\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True)\n",
    "        \n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "# Load Wine dataset\n",
    "wine_data = load_wine()\n",
    "X_wine = wine_data.data\n",
    "y_wine = wine_data.target\n",
    "\n",
    "# One-hot encoding for labels\n",
    "y_wine_onehot = np.eye(3)[y_wine]\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(X_wine, y_wine_onehot, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train and evaluate the Wine dataset\n",
    "mlp_wine = MLP(input_size=X_train_wine.shape[1], hidden_size=50, output_size=3)\n",
    "mlp_wine.train(X_train_wine, y_train_wine, epochs=10000)\n",
    "\n",
    "# Prediction\n",
    "y_pred_wine = mlp_wine.predict(X_test_wine)\n",
    "y_pred_wine_class = np.argmax(y_pred_wine, axis=1)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy_wine = accuracy_score(np.argmax(y_test_wine, axis=1), y_pred_wine_class)\n",
    "print(f\"Accuracy on Wine dataset: {accuracy_wine * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_wine = confusion_matrix(np.argmax(y_test_wine, axis=1), y_pred_wine_class)\n",
    "print(f\"Confusion Matrix for Wine dataset:\\n\", cm_wine)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(cm_wine, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix for Wine dataset\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e92b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Diabetes and Wine ReLu and Softmax\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# ReLU activation function and derivative\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Softmax activation function\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))  # For numerical stability\n",
    "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "# MLP Class with Backpropagation using Softmax\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = relu(self.hidden_input)  # Using ReLU for hidden layers\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output = softmax(self.output_input)  # Using Softmax for multi-class output\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        output_error = y - output\n",
    "        output_delta = output_error  # No softmax derivative as we are using a direct error\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * relu_derivative(self.hidden_output)\n",
    "        \n",
    "        self.weights_input_hidden += X.T.dot(hidden_delta)\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta)\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True)\n",
    "        \n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "# Load CSV data\n",
    "def load_csv_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Main function to handle both datasets\n",
    "def main():\n",
    "    # Load Wine dataset\n",
    "    wine_data = load_csv_data('wine.csv')  # Update the path\n",
    "    X_wine = wine_data.drop(columns=['class'])\n",
    "    y_wine = wine_data['class']\n",
    "\n",
    "    # Load Diabetes dataset\n",
    "    diabetes_data = load_csv_data('diabetes.csv')  # Update the path\n",
    "    X_diabetes = diabetes_data.drop(columns=['Outcome'])\n",
    "    y_diabetes = diabetes_data['Outcome']\n",
    "\n",
    "    # Preprocess and split data\n",
    "    X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(X_wine, y_wine, test_size=0.3, random_state=42)\n",
    "    X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = train_test_split(X_diabetes, y_diabetes, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Convert labels to one-hot for both datasets\n",
    "    y_train_wine_onehot = np.zeros((len(y_train_wine), 3))\n",
    "    y_train_wine_onehot[np.arange(len(y_train_wine)), y_train_wine - 1] = 1  # Adjust for Wine dataset labels starting from 1\n",
    "\n",
    "    y_train_diabetes_onehot = np.zeros((len(y_train_diabetes), 2))\n",
    "    y_train_diabetes_onehot[np.arange(len(y_train_diabetes)), y_train_diabetes] = 1\n",
    "\n",
    "    # Train and evaluate Wine dataset\n",
    "    mlp_wine = MLP(input_size=X_train_wine.shape[1], hidden_size=50, output_size=3)\n",
    "    mlp_wine.train(X_train_wine.values, y_train_wine_onehot, epochs=10000)\n",
    "\n",
    "    # Prediction for Wine dataset\n",
    "    y_pred_wine = mlp_wine.predict(X_test_wine.values)\n",
    "    y_pred_wine = np.argmax(y_pred_wine, axis=1) + 1  # Convert probabilities to class labels\n",
    "\n",
    "    # Calculate Accuracy for Wine dataset\n",
    "    accuracy_wine = accuracy_score(y_test_wine, y_pred_wine)\n",
    "    print(f\"Accuracy on Wine dataset: {accuracy_wine * 100:.2f}%\")\n",
    "\n",
    "    # Confusion Matrix for Wine dataset\n",
    "    cm_wine = confusion_matrix(y_test_wine, y_pred_wine)\n",
    "    print(f\"Confusion Matrix for Wine dataset:\\n\", cm_wine)\n",
    "\n",
    "    # Plot Confusion Matrix for Wine dataset\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    sns.heatmap(cm_wine, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_wine), yticklabels=np.unique(y_wine))\n",
    "    plt.title(\"Confusion Matrix for Wine dataset\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    # Train and evaluate Diabetes dataset\n",
    "    mlp_diabetes = MLP(input_size=X_train_diabetes.shape[1], hidden_size=50, output_size=2)\n",
    "    mlp_diabetes.train(X_train_diabetes.values, y_train_diabetes_onehot, epochs=10000)\n",
    "\n",
    "    # Prediction for Diabetes dataset\n",
    "    y_pred_diabetes = mlp_diabetes.predict(X_test_diabetes.values)\n",
    "    y_pred_diabetes = np.argmax(y_pred_diabetes, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "    # Calculate Accuracy for Diabetes dataset\n",
    "    accuracy_diabetes = accuracy_score(y_test_diabetes, y_pred_diabetes)\n",
    "    print(f\"Accuracy on Diabetes dataset: {accuracy_diabetes * 100:.2f}%\")\n",
    "\n",
    "    # Confusion Matrix for Diabetes dataset\n",
    "    cm_diabetes = confusion_matrix(y_test_diabetes, y_pred_diabetes)\n",
    "    print(f\"Confusion Matrix for Diabetes dataset:\\n\", cm_diabetes)\n",
    "\n",
    "    # Plot Confusion Matrix for Diabetes dataset\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(cm_diabetes, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "    plt.title(\"Confusion Matrix for Diabetes dataset\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
